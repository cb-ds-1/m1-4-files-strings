{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe dict reading\n",
    "\n",
    "define a function `safe_dict(d, k)` that takes in a python dict `d` and a key `k` and makes it safe to read even with keys that aren't in the dictionary. If you try to read from the dictionary with a bad key, it should return 0 instead.\n",
    "\n",
    "```\n",
    "d = {1 : 2, 3 : 4}\n",
    "safe_dict(d, 1) -> 2\n",
    "safe_dict(d, 'cat') -> 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {1: 2, 3: 4}\n",
    "\n",
    "def safe_dict(d, k):\n",
    "    try:\n",
    "        v = d[k]\n",
    "        return v\n",
    "    except KeyError:\n",
    "        return 0\n",
    "safe_dict(d, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tl_ZhkbEtiTD"
   },
   "source": [
    "# File Reading: Hamlet Exercises\n",
    "\n",
    "Open `hamlet.txt` in the `data` folder\n",
    "\n",
    "### 1. Mentionned Hamlet\n",
    "\n",
    "How many times is hamlet mentioned in the book?\n",
    "\n",
    "Use python and line iteration to count it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n"
     ]
    }
   ],
   "source": [
    "hamlet = open('data/hamlet.txt', \"r\")\n",
    "lines = hamlet.readlines()\n",
    "total = 0\n",
    "\n",
    "for line in lines:\n",
    "#     We check for each way to spell hamlet..\n",
    "    hamlets = line.count(\"Hamlet\")\n",
    "    hamlets2 = line.count(\"HAMLET\")\n",
    "    hamlets3 = line.count(\"hamlet\")\n",
    "#     Add up the occurences found\n",
    "    if hamlets > 0: total += hamlets\n",
    "    if hamlets2 > 0: total += hamlets2\n",
    "    if hamlets3 > 0: total += hamlets2\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. File Reading as a .py program\n",
    "\n",
    "Make a python file that defines a function that counts the number of times hamlet is mentionned using the code in the previous exercise.\n",
    "\n",
    "Then import it in your notebook and call it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n",
      "[Errno 2] No such file or directory: ''\n",
      "0\n",
      "Please provide at least one word to search for\n",
      "0\n",
      "Please provide a string for the file/filepath\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import fword\n",
    "\n",
    "print(fword.check_for('data/hamlet.txt', [\"Hamlet\", \"HAMLET\", \"hamlet\"]))\n",
    "print(fword.check_for('', [\"Hamlet\", \"HAMLET\", \"hamlet\"]))\n",
    "print(fword.check_for('data/hamlet.txt', []))\n",
    "print(fword.check_for(0,[\"Hamlet\", \"HAMLET\", \"hamlet\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Unique words in hamlet\n",
    "\n",
    "Write a program that counts the unique words in hamlet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120\n"
     ]
    }
   ],
   "source": [
    "import fword\n",
    "\n",
    "print(fword.no_unique_words_in('data/hamlet.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Reading 2: A Python library.\n",
    "\n",
    "In the `data` folder, you will find a folder called `csrgraph` which is a python library.\n",
    "\n",
    "### 1. File count\n",
    "\n",
    "Count the `py` files in the library using the `os` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 files in the data/csrgraph folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def files_in(path):\n",
    "    content = []\n",
    "    try: return os.listdir(path)\n",
    "    except Exception as e: raise e\n",
    "\n",
    "def how_many_files_in(path):\n",
    "    content = []\n",
    "    try:\n",
    "        content = files_in(path)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return\n",
    "    print(\"There are\", len(content), \"files in the\", path, \"folder\")\n",
    "\n",
    "how_many_files_in('data/csrgraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. For the following packages, count the number of files that import them:\n",
    "\n",
    "- pandas \n",
    "\n",
    "- numpy\n",
    "\n",
    "- numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'os': 2, 'pandas': 4, 'numpy': 6, 'numba': 5}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fword\n",
    "\n",
    "# returns an array of file names contained in the specified path\n",
    "# def files_in(path):\n",
    "#     try: return os.listdir(path)\n",
    "#     except Exception as e: raise e\n",
    "        \n",
    "# returns a dictionary with the keys as the file names,\n",
    "# and the values as arrays of the lines of the file\n",
    "# def files_as_lines_in(path):\n",
    "#     try: \n",
    "#         file_names = files_in(path)\n",
    "#         return dict(zip(file_names, [fword.get_all_lines_in_file(path+\"/\"+file) for file in file_names]))\n",
    "#     except Exception as e: \n",
    "#         raise e\n",
    "      \n",
    "# Checks whether a file has a word or not\n",
    "# def does_file_have_word(file_path, word): return fword.check_for(file_path, [word]) > 0\n",
    "\n",
    "# Checks wether a set of files contains a specific word\n",
    "# def files_containing_word_at_path(in_path, file_set, word):\n",
    "#     total = 0\n",
    "#     for file in files: total += 1 if does_file_have_word(in_path+\"/\"+file, \"import \"+ pkge) else 0\n",
    "#     return total\n",
    "\n",
    "\n",
    "def no_of_files_containing_word(files, word):\n",
    "    if isinstance(files, dict) is False: \n",
    "        raise Error(\"Error no_of_files_containing_word - files must be a dictionary\")\n",
    "    total = 0\n",
    "    for file in files.keys(): total += 1 if fword.do_lines_have_word(files[file], word) else 0 \n",
    "    return total\n",
    "        \n",
    "\n",
    "def no_of_files_importing(pkges, in_path):\n",
    "    \n",
    "#     we create a dictionary of the packages, initialized at 0\n",
    "    if len(pkges) < 1: return 0\n",
    "    if in_path is None: return 0\n",
    "    packages = dict(zip(pkges, [0] * len(pkges)))\n",
    "    \n",
    "#     we fetch an array of the file names at the path specified\n",
    "    files = {}\n",
    "    try:\n",
    "        files = fword.files_as_lines_in(in_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "#     we iterate over each package and check each file\n",
    "    for pkge in pkges: \n",
    "        # we check for packages that are empty strings\n",
    "        if len(pkge) <= 1: \n",
    "            print(\"Package '\", pkge, \"' is not a proper package name\")\n",
    "            return\n",
    "        else:\n",
    "            packages[pkge] += no_of_files_containing_word(files, \"import \"+pkge)\n",
    "    return packages\n",
    "\n",
    "print(no_of_files_importing([\"os\", \"pandas\", \"numpy\", \"numba\"], \"data/csrgraph\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First NLP Program: IDF\n",
    "\n",
    "Given a list of words, the the inverse document frequency (IDF) is a basic statistic of the amount of information of each word in the text.\n",
    "\n",
    "The IDF formulat is:\n",
    "\n",
    "$$IDF(w) = ln(\\dfrac{N}{1 + n(w)})$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w$ is the token (unique word),\n",
    "- $n(w)$ is the number of documents that $w$ occurs in,\n",
    "- $N$ is the total number of documents\n",
    "\n",
    "Write a function, `idf(docs)` that takes in a list of lists of words and returns a dictionary  `word -> idf score`\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "IDF([['interview', 'questions'], ['interview', 'answers']]) -> {'questions': 0.0, \n",
    "                                                                'interview': -0.4, \n",
    "                                                                'answers': 0.0}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': 1, 'questions': 0, 'interview': 0}\n",
      "{'answers': 1, 'questions': 1, 'interview': 0}\n",
      "{'answers': 1, 'questions': 1, 'interview': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answers': 0.0, 'questions': 0.0, 'interview': -0.40546510810816444}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "import fword\n",
    "\n",
    "def idf(docs):\n",
    "    \n",
    "    # first build a a dict <doc_index:unique_words_set_for_doc>\n",
    "    # doc_name will be the doc_index in the docs array\n",
    "    doc_dict = {}\n",
    "    unique_words = set()\n",
    "    \n",
    "    # we populate the dict\n",
    "    for i in range(len(docs)):\n",
    "        doc_dict[str(i)] = set(docs[i])\n",
    "        # at the same time we get the set of unique words for all docs\n",
    "        unique_words.update(doc_dict[str(i)])\n",
    "    \n",
    "#     we make one counter dictionary <word:counter> to keep track of each word's occurence\n",
    "    words_in_no_of_docs = dict(zip([str(word) for word in unique_words], [0] * len(unique_words)))\n",
    "#     we clone this base object for the result to return\n",
    "    result = dict(words_in_no_of_docs)\n",
    "\n",
    "    for word in unique_words:\n",
    "#     for each word in unique_words we check how many times it appears in each doc\n",
    "        for doc in doc_dict.keys(): \n",
    "            words_in_no_of_docs[word] += 1 if word in doc_dict[doc] else 0\n",
    "#       we then apply the formulae to the result dict for each word\n",
    "        print(words_in_no_of_docs)\n",
    "        result[word] = math.log((len(docs) / (1 + words_in_no_of_docs[word])))\n",
    "    return result\n",
    "\n",
    "    \n",
    "idf([['interview', 'questions'], ['interview', 'answers']])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "82bfnc_KueoX"
   },
   "source": [
    "# Stretch Goal: TF-IDF on Hamlet\n",
    "\n",
    "The TF-IDF score is a commonly used statistic for the importance of words. Its $\\frac{TF}{IDF}$ where TF is the \"term frequency\" (eg. how often the words happens in the document).\n",
    "\n",
    "Calculate the TF-IDF dictionary on the Hamlet book.\n",
    "\n",
    "What's the TF-IDF of \"Hamlet\"?\n",
    "\n",
    "What's the word with the highest TF-IDF in the book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import fword\n",
    "\n",
    "\n",
    "def idf(docs):\n",
    "    \n",
    "    # first build a a dict <doc_index:unique_words_set_for_doc>\n",
    "    # doc_name will be the doc_index in the docs array\n",
    "    doc_dict = {}\n",
    "    unique_words = set()\n",
    "    \n",
    "    # we populate the dict\n",
    "    for i in range(len(docs)):\n",
    "        doc_dict[str(i)] = set(docs[i])\n",
    "        # at the same time we get the set of unique words for all docs\n",
    "        unique_words.update(doc_dict[str(i)])\n",
    "    \n",
    "#     we make one counter dictionary <word:counter> to keep track of each word's occurence\n",
    "    words_in_no_of_docs = dict(zip([str(word) for word in unique_words], [0] * len(unique_words)))\n",
    "#     we clone this base object for the result to return\n",
    "    result = dict(words_in_no_of_docs)\n",
    "    \n",
    "\n",
    "    for word in unique_words:\n",
    "#     for each word in unique_words we check how many times it appears in each doc\n",
    "        for doc in doc_dict.keys(): \n",
    "            words_in_no_of_docs[word] += 1 if word in doc_dict[doc] else 0\n",
    "#       we then apply the formulae to the result dict for each word\n",
    "        result[word] = math.log((len(docs) / (1 + words_in_no_of_docs[word])))\n",
    "    return result\n",
    "\n",
    "def td_idf(file):\n",
    "    # This method takes a filepath, converts it to an array of words only\n",
    "    # [[word, word], [word, word]]\n",
    "    lines = fword.get_all_lines_in_file_as_array_of_words_only(file)\n",
    "    idf_file = idf(lines)\n",
    "    tf_file = {}\n",
    "    result = {}\n",
    "    highest_v = -10000\n",
    "    highest_n = \"\"\n",
    "    for word in idf_file.keys():\n",
    "#         This method counts the number of times the word appears in all lines\n",
    "        tf_file[word] = fword.no_of_times_word_occurs_in(lines, word)\n",
    "#     Calculate the tf-idf for the word\n",
    "        result[word] = tf_file[word] / idf_file[word]\n",
    "#     We update a max-value tracker pair of variables\n",
    "        if result[word] > highest_v:\n",
    "            highest_n = word\n",
    "            highest_v = result[word]\n",
    "\n",
    "    # we could have done some extra work to consider all-caps, capitalized an non-capitalized \n",
    "    # versions of each word as well (ie: the and The)\n",
    "    print(idf_file)\n",
    "    print(\"the TF-IDF of Hamlet is\", result[\"Hamlet\"])\n",
    "    print(\"the TF-IDF of HAMLET is\", result[\"HAMLET\"])\n",
    "    print(\"the word with the highest TF-IDF is '\", highest_n, \"' with, (\", highest_v,\")\")\n",
    "    \n",
    "td_idf('data/hamlet.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stretch Goal: Speaker count\n",
    "\n",
    "Use a regular expression and looping over the `hamlet.txt` file to build a dictionary `character_name -> # times speaking`.\n",
    "\n",
    "Who speaks the most often? Who speaks the least often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fword\n",
    "\n",
    "\n",
    "def speaker_count(file):\n",
    "    #we want to check lines of words only to count names\n",
    "    #speakers are \"SPEAKER_NAME.\" \n",
    "    #without punctuation, we only have a one-word line in all caps\n",
    "    #since the method get_all_lines_in_file_as_array_of_words_only(file)\n",
    "    #of our library already filters out punctuation and spaces, and since\n",
    "    #speaker name lines are always in caps,\n",
    "    #if we check lines that only have one word and are all-caps we can be sure we're\n",
    "    #looking at a speaker header\n",
    "    \n",
    "    lines = fword.get_all_lines_in_file_as_array_of_words_only(file)\n",
    "    speakers = {}\n",
    "    speaker_n_max = 0\n",
    "    speaker_name = \"\"\n",
    "    rege = re.compile(r'^[A-Z\\d]+$')\n",
    "    for line in lines:\n",
    "        if len(line) == 1 and rege.match(line[0]):\n",
    "            if line[0] in speakers: speakers[line[0]] += 1\n",
    "            else: speakers[line[0]] = 1\n",
    "            if speakers[line[0]] > speaker_n_max: \n",
    "                speaker_n_max = speakers[line[0]]\n",
    "                speaker_name = line[0]\n",
    "    print(speakers)\n",
    "    print(\"The character who speaks the most is:\", speaker_name, \"(\", speaker_n_max, \"times )\")\n",
    "                \n",
    "speaker_count(\"data/hamlet.txt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPMuPsz+efoYpJzg8ElS0Ut",
   "collapsed_sections": [],
   "name": "Workshop Python Intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
